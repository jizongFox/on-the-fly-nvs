#
# Copyright (C) 2025, Inria
# GRAPHDECO research group, https://team.inria.fr/graphdeco
# All rights reserved.
#
# This software is free for non-commercial, research and evaluation use
# under the terms of the LICENSE.md file.
#
# For inquiries contact  george.drettakis@inria.fr
#

import torch
import torch.nn as nn
import torch.nn.functional as F
import os

from poses.matcher import Matches


class DescribedKeypoints:
    """
    A class to store 2D keypoints, their descriptors, their matches to other images, and their estimated 3D positions.
    """

    @torch.no_grad()
    def __init__(self, kpts, feats):
        self.kpts = kpts
        self.feats = feats
        self.valid = feats.abs().sum(dim=-1) > 0
        self.nvalid = self.valid.sum()
        self.has_pt3d = torch.zeros(kpts.shape[0], dtype=torch.bool, device=kpts.device)
        self.pts_conf = torch.zeros(
            kpts.shape[0], dtype=torch.float, device=kpts.device
        )
        self.pts3d = torch.zeros(
            kpts.shape[0], 3, dtype=torch.float, device=kpts.device
        )
        self.depth = torch.zeros(kpts.shape[0], dtype=torch.float, device=kpts.device)
        self.matches = {}

    @torch.no_grad()
    def update_matches(self, other_id, matches, swap=False):
        if swap:
            matches = Matches(
                matches.kpts_other, matches.kpts, matches.idx_other, matches.idx
            )
        self.matches[other_id] = matches

    @torch.no_grad()
    def update_3D_pts(self, pts3D, depth, conf, idx):
        self.has_pt3d[idx] = True
        self.depth[idx] = depth
        self.pts3d[idx] = pts3D
        self.pts_conf[idx] = conf

    @torch.no_grad()
    def to(self, device):
        self.kpts = self.kpts.to(device)
        self.feats = self.feats.to(device)
        self.valid = self.valid.to(device)
        self.has_pt3d = self.has_pt3d.to(device)
        self.pts_conf = self.pts_conf.to(device)
        self.pts3d = self.pts3d.to(device)
        self.depth = self.depth.to(device)
        for key in self.matches:
            self.matches[key].kpts = self.matches[key].kpts.to(device)
            self.matches[key].kpts_other = self.matches[key].kpts_other.to(device)
            self.matches[key].idx = self.matches[key].idx.to(device)
            self.matches[key].idx_other = self.matches[key].idx_other.to(device)


# From https://github.com/verlab/accelerated_features
# XFeat: Accelerated Features for Lightweight Image Matching, Under Apache-2.0 license
class InterpolateSparse2d(nn.Module):
    """Efficiently interpolate tensor at given sparse 2D positions."""

    def __init__(self, mode="bicubic", align_corners=False):
        super().__init__()
        self.mode = mode
        self.align_corners = align_corners

    def normgrid(self, x, H, W):
        """Normalize coords to [-1,1]."""
        return (
            2.0 * (x / (torch.tensor([W - 1, H - 1], device=x.device, dtype=x.dtype)))
            - 1.0
        )

    def forward(self, x, pos, H, W):
        """
        Input
            x: [B, C, H, W] feature tensor
            pos: [B, N, 2] tensor of positions
            H, W: int, original resolution of input 2d positions -- used in normalization [-1,1]

        Returns
            [B, N, C] sampled channels at 2d positions
        """
        grid = self.normgrid(pos, H, W).unsqueeze(-2).to(x.dtype)
        x = F.grid_sample(x, grid, mode=self.mode, align_corners=False)
        return x.permute(0, 2, 3, 1).squeeze(-2)


class Detector:
    @torch.no_grad()
    def __init__(self, top_k, width, height):
        cache_path = f"models/cache/xfeat_{width}_{height}_{top_k}.pt"
        dummy_img = torch.randn(1, 3, height, width).cuda().to(torch.half)
        if os.path.exists(cache_path):
            extractor = torch.jit.load(cache_path)
        else:
            print(f"Compiling feature extractor")
            extractor = torch.hub.load(
                "verlab/accelerated_features", "XFeat", pretrained=True, top_k=top_k
            )
            extractor = extractor.cuda().eval().to(torch.half)

            ## Overriding the functions to run at fixed size

            # Adapted from https://github.com/verlab/accelerated_features to enable jit tracing
            # XFeat: Accelerated Features for Lightweight Image Matching, Under Apache-2.0 license
            def preprocess_tensor(x):
                H, W = x.shape[-2:]
                _H, _W = (H // 32) * 32, (W // 32) * 32
                rh, rw = H / _H, W / _W

                x = F.interpolate(x, (_H, _W), mode="bilinear", align_corners=False)
                return x, rh, rw

            # Adapted from https://github.com/verlab/accelerated_features to enable jit tracing
            # XFeat: Accelerated Features for Lightweight Image Matching, Under Apache-2.0 license
            def NMS(x, threshold=0.05, kernel_size=5, nvalid=int(1.5 * top_k)):
                B, _, H, W = x.shape
                pad = kernel_size // 2
                local_max = nn.MaxPool2d(
                    kernel_size=kernel_size, stride=1, padding=pad
                )(x)
                xTot = x * (x == local_max) * (x > threshold)
                xOut, pos1d = torch.topk(xTot.view(-1), nvalid)
                pos2d = torch.zeros((B, nvalid, 2), dtype=torch.long, device=x.device)
                pos2d[..., 0] = pos1d % W
                pos2d[..., 1] = pos1d // W
                return xOut, pos2d

            # Adapted from https://github.com/verlab/accelerated_features to enable jit tracing
            # XFeat: Accelerated Features for Lightweight Image Matching, Under Apache-2.0 license
            def detectAndCompute(x):
                top_k = extractor.top_k
                detection_threshold = extractor.detection_threshold
                x, rh1, rw1 = extractor.preprocess_tensor(x)

                B, _, _H1, _W1 = x.shape

                M1, K1, H1 = extractor.net(x)
                M1 = F.normalize(M1, dim=1)

                # Convert logits to heatmap and extract kpts
                K1h = extractor.get_kpts_heatmap(K1)
                xOut, mkpts = extractor.NMS(
                    K1h, threshold=detection_threshold, kernel_size=5
                )

                # Compute reliability scores
                _nearest = InterpolateSparse2d("nearest")
                _bilinear = InterpolateSparse2d("bilinear")
                scores = (
                    _nearest(K1h, mkpts, _H1, _W1) * _bilinear(H1, mkpts, _H1, _W1)
                ).squeeze(-1)
                scores[torch.all(mkpts == 0, dim=-1)] = -1

                # Select top-k features
                # idxs = torch.argsort(-scores)
                idxs = torch.topk(scores, top_k)[1]
                mkpts_x = torch.gather(mkpts[..., 0], -1, idxs)
                mkpts_y = torch.gather(mkpts[..., 1], -1, idxs)
                xOut = torch.gather(xOut[None], -1, idxs)
                mkpts = torch.cat([mkpts_x[..., None], mkpts_y[..., None]], dim=-1)
                scores = torch.gather(scores, -1, idxs)
                scores *= xOut > 0

                # Interpolate descriptors at kpts positions
                feats = extractor.interpolator(M1, mkpts, H=_H1, W=_W1)

                # L2-Normalize
                feats = F.normalize(feats, dim=-1)

                # Correct kpt scale
                mkpts = mkpts.float() * torch.tensor(
                    [rw1, rh1], device=mkpts.device, dtype=torch.float
                ).view(1, 1, -1)

                return mkpts[0], feats[0] * (scores[0][..., None] > 0)

            extractor.preprocess_tensor = preprocess_tensor
            extractor.NMS = NMS
            extractor.forward = detectAndCompute

            extractor = torch.jit.trace(extractor, [dummy_img])
            os.makedirs(os.path.dirname(cache_path), exist_ok=True)
            torch.jit.save(extractor, cache_path)
            extractor = torch.jit.load(cache_path)

        self.extractor = extractor

    @torch.no_grad()
    def __call__(self, image):
        return DescribedKeypoints(*(self.extractor(image[None].half())))
